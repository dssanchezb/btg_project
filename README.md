# ğŸ§  BTG Energy DataLake Project

Este proyecto representa una soluciÃ³n analÃ­tica basada en AWS para una compaÃ±Ã­a comercializadora de energÃ­a. La compaÃ±Ã­a administra informaciÃ³n sobre **proveedores, clientes y transacciones**, que se exportan desde sus sistemas como archivos CSV.

El objetivo es construir una arquitectura de procesamiento de datos moderna y escalable sobre AWS utilizando servicios como **S3, AWS Glue, Athena y Terraform**.

---

## ğŸ“Œ Enunciado

Una compaÃ±Ã­a comercializadora de energÃ­a compra la electricidad a los generadores y luego la vende a usuarios finales (residenciales, comerciales o industriales). La compaÃ±Ã­a puede exportar archivos CSV con informaciÃ³n de:

- Proveedores  
- Clientes  
- Transacciones

---

## âš™ï¸ Requisitos TÃ©cnicos

1. âœ… **Estrategia de Datalake** en Amazon S3, estructurada en capas y con almacenamiento particionado por fecha.
2. âœ… **Transformaciones** bÃ¡sicas utilizando AWS Glue. La salida se almacena como archivos **Parquet** en la zona procesada.
3. âœ… **Crawler automÃ¡tico** de AWS Glue para detectar y catalogar los datos en cada nueva carga.
4. âœ… **Consultas SQL en Athena** desde Python, para extraer insights de los datos transformados.

---

## ğŸ—‚ï¸ Estructura del Proyecto
```text
ğŸ“ BTG_PROJECT/
   â””â”€â”€ data/ # Carpeta local donde residen archivos CSV de ejemplo
   â””â”€â”€ glue/
   | â””â”€â”€ btg_transformation.py # Script PySpark con las transformaciones en Glue
   â””â”€â”€ iac/
   | â””â”€â”€  main.tf # Infraestructura como cÃ³digo con Terraform
   â””â”€â”€ .gitignore
   â””â”€â”€ README.md
```
---

## ğŸ—ï¸ Arquitectura

```
ğŸ“ Raw Layer (S3 bucket)
   â””â”€â”€ proveedores/
   â””â”€â”€ clientes/
   â””â”€â”€ transacciones/

ğŸ”„ AWS Glue Crawler
   â””â”€â”€ Detecta y cataloga nuevos archivos

ğŸ§ª AWS Glue Job
   â””â”€â”€ Realiza transformaciones bÃ¡sicas en PySpark

ğŸ“ Processed Layer (S3 bucket)
   â””â”€â”€ parquet/proveedores/
   â””â”€â”€ parquet/clientes/
   â””â”€â”€ parquet/transacciones/

ğŸ” Amazon Athena
   â””â”€â”€ SQL sobre las tablas catalogadas
```

---

## ğŸ”„ Transformaciones Realizadas

1. Las transformaciones bÃ¡sicas implementadas en btg_transformation.py incluyen:
2. Limpieza de datos nulos o mal formateados.
3. ConversiÃ³n de tipos de datos adecuados.
4. Escritura de los resultados como archivos Parquet particionados por year y month.

---

## ğŸš€ Despliegue

### 1. Inicializa la infraestructura

```bash
cd iac
terraform init
terraform apply
```

Esto crearÃ¡:

- Buckets en S3
- Glue Crawlers
- Glue Jobs
- Tablas en Glue Data Catalog

### 2. Sube tus archivos CSV

Coloca los archivos CSV de `clientes`, `proveedores`, `transacciones` en la ruta `s3://<bucket>/raw/<tabla>/year=YYYY/month=MM/`.

### 3. Ejecuta los procesos

Puedes ejecutar manualmente el **Glue Job** o crear un **Trigger** periÃ³dico desde consola.

TambiÃ©n puedes ejecutar consultas desde Athena o desde Python con `boto3` o `PyAthena`.

---

## ğŸ§  Consultas Athena desde Python

Se pueden realizar consultas bÃ¡sicas como: detalle de transacciones realizadas por cliente.

```sql
select 
    c.*,
    t.transaccion_id,
    tipo_transaccion,
    cantidad_comprada,
    t.precio,
    t.tipo_energia,
    t.year,
    t.month 
from transacciones_transacciones t
join clientes_clientes c on t.id_cliente_proveedor = c.cliente_id
;
```

---

## ğŸ› ï¸ TecnologÃ­as Utilizadas

- **AWS S3**: Almacenamiento de datos en capas
- **AWS Glue**: Transformaciones, catalogaciÃ³n, crawlers
- **AWS Athena**: Motor SQL serverless para anÃ¡lisis
- **Terraform**: Infraestructura como cÃ³digo
- **Python + PySpark**: Procesamiento de datos

---

## ğŸ“ Notas Finales

- Todo el procesamiento estÃ¡ particionado por `year` y `month` de carga.
- El Glue Crawler se actualiza automÃ¡ticamente al detectar nuevos archivos.
- Las transformaciones son idempotentes: no sobrescriben datos antiguos.

---

## ğŸ§‘â€ğŸ’» Autor

Este proyecto fue desarrollado como ejercicio tÃ©cnico de arquitectura de datos sobre AWS.